import json
import asyncio
from datetime import datetime
import httpx
from fastapi import APIRouter, Request, HTTPException
from fastapi.responses import JSONResponse, StreamingResponse
from utils import check_backend_health, parse_model_id
from config import LM_STUDIO_BASE_URL, API_TIMEOUT

router = APIRouter()

@router.post("/api/pull")
async def handle_pull():
    """Fakes the 'pull' command from Ollama clients to improve compatibility."""
    print("--- Received /api/pull request. Faking success. ---")
    async def fake_pull_stream():
        yield json.dumps({"status": "pulling digest"}) + "\n"
        await asyncio.sleep(0.1)
        yield json.dumps({"status": "success"}) + "\n"
    return StreamingResponse(fake_pull_stream(), media_type="application/x-ndjson")

@router.post("/api/show")
async def handle_ollama_show(request: Request):
    """Returns dummy information about a model to satisfy client requests."""
    try:
        ollama_data = await request.json()
        model_name = ollama_data.get("name")
        print(f"--- Received /api/show request for model: {model_name} ---")

        # Return a dummy response, as we don't have detailed model info.
        response_data = {
            "license": "",
            "modelfile": "# Modelfile generated by Ollama Shim",
            "parameters": "",
            "template": "",
            "details": {
                "family": "",
                "format": "gguf",
                "parameter_size": "",
                "quantization_level": ""
            }
        }

        return JSONResponse(content=response_data)

    except Exception as e:
        print(f"An error occurred in /api/show: {e}")
        return JSONResponse(status_code=500, content={
            "error": {
                "message": str(e),
                "code": 500,
                "details": f"Ollama Shim service encountered an unexpected error"
            }
        })

@router.get("/api/tags")
async def handle_tags():
    """
    Fetches the list of available models from the LM Studio backend and
    translates it to the Ollama-compatible format.
    """
    print("--- Received /api/tags request. Checking LM Studio for models. ---")

    models_url = f"{LM_STUDIO_BASE_URL.rstrip('/')}/v1/models"
    try:
        # Check backend availability before calling
        if not (await check_backend_health()):
            raise HTTPException(
                status_code=503,
                detail={
                    "error": {
                        "message": "Model service unavailable",
                        "code": 503,
                        "suggested_action": "Check LM Studio server at " + LM_STUDIO_BASE_URL
                    }
                }
            )

        async with httpx.AsyncClient(timeout=API_TIMEOUT) as client:
            response = await asyncio.wait_for(client.get(models_url), timeout=API_TIMEOUT)
            response.raise_for_status()
            lm_studio_models_data = response.json()
        # Transform the OpenAI-style response to Ollama-style
        ollama_models = []
        for model in lm_studio_models_data.get("data", []):
            model_id = model.get("id")
            if model_id:
                modified_time = datetime.fromtimestamp(model.get("created", 0)).isoformat() + "Z"
                family = model_id.split('-')[0] if '-' in model_id else model_id

                parsed_details = parse_model_id(model_id)

                ollama_models.append({
                    "name": model_id,
                    "model": model_id,
                    "modified_at": modified_time,
                    "size": 0,  # Size info is not available from LM Studio's API
                    "digest": model.get("id"),  # Use ID as a fake digest
                    "details": {
                        "format": "gguf",
                        "family": family,
                        "families": [family],
                        "parameter_size": parsed_details["parameter_size"],
                        "quantization_level": parsed_details["quantization_level"]
                    }
                })

        response_data = {"models": ollama_models}
        print(f"--- Responding to /api/tags with {len(ollama_models)} model(s). ---")
        return JSONResponse(content=response_data)

    except HTTPException:
        raise
    except (httpx.RequestError, asyncio.TimeoutError) as e:
        error_msg = f"Failed to connect to model service at {models_url}: {str(e)}"
        print(error_msg)
        return JSONResponse(
            status_code=502,
            content={
                "error": {
                    "message": "Model service unavailable",
                    "code": 502,
                    "details": error_msg
                }
            }
        )
    except Exception as e:
        print(f"An error occurred in /api/tags: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "error": {
                    "message": str(e),
                    "code": 500,
                    "details": f"Ollama Shim service encountered an unexpected error"
                }
            }
        )